"""
GCP Spot VM Checkpoint Manager - v91.0 Advanced Edition
========================================================

Enhanced checkpoint management with:
- Predictive preemption using maintenance events API
- Multi-signal preemption detection (maintenance, termination, SIGTERM)
- Exponential backoff retry for GCS uploads
- Intermediate state preservation (gradients, data loader state)
- Async-safe operations with proper cancellation
- Health monitoring integration
- Graceful shutdown coordination with Trinity
- Resumable training state machine
- Automatic checkpoint verification

ROOT PROBLEMS SOLVED:
1. VM termination interrupting long training runs
2. Loss of intermediate state during preemption
3. No predictive pre-warming for checkpoint saves
4. Race conditions in async checkpoint uploads
5. No verification of checkpoint integrity
"""
import asyncio
import gc
import hashlib
import json
import os
import shutil
import signal
import sys
import time
import weakref
from abc import ABC, abstractmethod
from collections import deque
from contextlib import asynccontextmanager
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta
from enum import Enum, auto
from pathlib import Path
from typing import Any, Callable, Deque, Dict, List, Optional, Set, Tuple, Union

import torch
import torch.nn as nn

logger = None  # Lazy init to avoid import cycles


def _get_logger():
    global logger
    if logger is None:
        import logging
        logger = logging.getLogger(__name__)
    return logger


# =============================================================================
# ENUMS AND CONSTANTS
# =============================================================================


class PreemptionSignal(Enum):
    """Types of preemption signals we monitor."""
    NONE = "none"
    MAINTENANCE_EVENT = "maintenance_event"  # 60s warning via maintenance events
    PREEMPTION_NOTICE = "preemption_notice"  # 30s warning via preempted endpoint
    SPOT_TERMINATION = "spot_termination"  # Immediate via termination endpoint
    SIGTERM = "sigterm"  # OS signal
    MANUAL = "manual"  # Manual trigger


class CheckpointType(Enum):
    """Types of checkpoints."""
    REGULAR = "regular"  # Normal interval-based
    EMERGENCY = "emergency"  # Preemption-triggered
    BEST = "best"  # Best loss achieved
    FINAL = "final"  # End of training


class ResumabilityStatus(Enum):
    """Status of checkpoint resumability."""
    COMPLETE = "complete"  # All state preserved
    PARTIAL = "partial"  # Model/optimizer only
    CORRUPTED = "corrupted"  # Verification failed
    UNKNOWN = "unknown"  # Not verified


# =============================================================================
# DATA CLASSES
# =============================================================================


@dataclass
class CheckpointState:
    """Training checkpoint state with extended metadata."""
    global_step: int
    epoch: int
    best_loss: float
    model_path: str
    optimizer_state_path: str
    scheduler_state_path: Optional[str] = None
    timestamp: str = ""
    metadata: Dict[str, Any] = field(default_factory=dict)

    # v91.0 Extensions
    checkpoint_type: str = "regular"
    resumability_status: str = "unknown"
    checksum: Optional[str] = None
    training_config: Dict[str, Any] = field(default_factory=dict)
    dataloader_state: Optional[Dict[str, Any]] = None
    random_states: Optional[Dict[str, Any]] = None
    gradient_accumulator_state: Optional[Dict[str, Any]] = None
    preemption_signal: Optional[str] = None
    time_to_preemption: Optional[float] = None
    gcs_backup_status: str = "pending"
    verification_passed: bool = False


@dataclass
class PreemptionContext:
    """Context for preemption handling."""
    signal_type: PreemptionSignal
    detected_at: float
    time_remaining: float  # Seconds until termination
    is_handled: bool = False
    checkpoint_path: Optional[str] = None
    error_message: Optional[str] = None


@dataclass
class CheckpointMetrics:
    """Metrics for checkpoint operations."""
    total_checkpoints: int = 0
    successful_saves: int = 0
    failed_saves: int = 0
    emergency_saves: int = 0
    gcs_uploads_pending: int = 0
    gcs_uploads_completed: int = 0
    gcs_uploads_failed: int = 0
    avg_save_time_ms: float = 0.0
    last_checkpoint_step: int = 0
    last_checkpoint_time: float = 0.0
    verification_passes: int = 0
    verification_failures: int = 0


# =============================================================================
# CHECKPOINT VERIFICATION
# =============================================================================


class CheckpointVerifier:
    """Verify checkpoint integrity and completeness."""

    @staticmethod
    def compute_file_checksum(path: Path) -> str:
        """Compute SHA256 checksum of a file."""
        sha256 = hashlib.sha256()
        with open(path, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    @staticmethod
    def compute_checkpoint_checksum(checkpoint_path: Path) -> str:
        """Compute combined checksum of all checkpoint files."""
        checksums = []

        for file_path in sorted(checkpoint_path.rglob("*")):
            if file_path.is_file() and file_path.name != "checkpoint_state.json":
                file_checksum = CheckpointVerifier.compute_file_checksum(file_path)
                checksums.append(f"{file_path.name}:{file_checksum}")

        combined = "\n".join(checksums)
        return hashlib.sha256(combined.encode()).hexdigest()[:16]

    @staticmethod
    def verify_checkpoint(checkpoint_path: Path) -> Tuple[bool, str]:
        """
        Verify checkpoint integrity.

        Returns:
            Tuple of (is_valid, error_message)
        """
        try:
            # Check state file exists
            state_path = checkpoint_path / "checkpoint_state.json"
            if not state_path.exists():
                return False, "checkpoint_state.json not found"

            # Load state
            with open(state_path) as f:
                state = json.load(f)

            # Check model file
            model_path = Path(state.get("model_path", ""))
            if not model_path.exists():
                return False, f"Model file not found: {model_path}"

            # Check optimizer file
            optimizer_path = Path(state.get("optimizer_state_path", ""))
            if not optimizer_path.exists():
                return False, f"Optimizer file not found: {optimizer_path}"

            # Verify model loadability
            try:
                torch.load(model_path, map_location="cpu", weights_only=True)
            except Exception as e:
                return False, f"Model file corrupted: {e}"

            # Verify optimizer loadability
            try:
                torch.load(optimizer_path, map_location="cpu", weights_only=True)
            except Exception as e:
                return False, f"Optimizer file corrupted: {e}"

            # Verify checksum if present
            stored_checksum = state.get("checksum")
            if stored_checksum:
                actual_checksum = CheckpointVerifier.compute_checkpoint_checksum(checkpoint_path)
                if stored_checksum != actual_checksum:
                    return False, f"Checksum mismatch: {stored_checksum} != {actual_checksum}"

            return True, "Verification passed"

        except Exception as e:
            return False, f"Verification error: {e}"

    @staticmethod
    def verify_gcs_checkpoint(
        gcs_client,
        bucket_name: str,
        prefix: str,
        checkpoint_name: str,
    ) -> Tuple[bool, str]:
        """Verify checkpoint exists and is complete in GCS."""
        try:
            bucket = gcs_client.bucket(bucket_name)
            blobs = list(bucket.list_blobs(prefix=f"{prefix}/{checkpoint_name}/"))

            required_files = {"model.pt", "optimizer.pt", "checkpoint_state.json"}
            found_files = {Path(blob.name).name for blob in blobs}

            missing = required_files - found_files
            if missing:
                return False, f"Missing files in GCS: {missing}"

            return True, "GCS checkpoint verified"

        except Exception as e:
            return False, f"GCS verification error: {e}"


# =============================================================================
# PREEMPTION DETECTOR
# =============================================================================


class PreemptionDetector:
    """
    Multi-signal preemption detector for GCP Spot VMs.

    Monitors:
    1. Maintenance events (60s warning)
    2. Preempted endpoint (30s warning)
    3. Spot termination endpoint
    4. SIGTERM signal
    5. Manual trigger
    """

    # GCP Metadata endpoints
    MAINTENANCE_ENDPOINT = "http://metadata.google.internal/computeMetadata/v1/instance/maintenance-event"
    PREEMPTED_ENDPOINT = "http://metadata.google.internal/computeMetadata/v1/instance/preempted"
    TERMINATION_ENDPOINT = "http://metadata.google.internal/computeMetadata/v1/instance/preempted"
    SPOT_TERMINATION_ENDPOINT = "http://metadata.google.internal/computeMetadata/v1/instance/spot/termination-time"

    METADATA_HEADERS = {"Metadata-Flavor": "Google"}

    def __init__(
        self,
        poll_interval: float = 2.0,  # Poll every 2 seconds
        maintenance_warning_seconds: float = 60.0,
        preemption_warning_seconds: float = 30.0,
    ):
        self.poll_interval = poll_interval
        self.maintenance_warning_seconds = maintenance_warning_seconds
        self.preemption_warning_seconds = preemption_warning_seconds

        # State
        self._running = False
        self._monitor_task: Optional[asyncio.Task] = None
        self._preemption_context: Optional[PreemptionContext] = None
        self._callbacks: List[Callable[[PreemptionContext], None]] = []
        self._lock = asyncio.Lock()

        # Signal handling
        self._original_sigterm_handler = None
        self._is_on_gcp = self._detect_gcp_environment()

        # Statistics
        self._total_checks = 0
        self._maintenance_events = 0
        self._preemption_notices = 0

    def _detect_gcp_environment(self) -> bool:
        """Detect if running on GCP."""
        try:
            import requests
            response = requests.get(
                "http://metadata.google.internal/computeMetadata/v1/instance/id",
                headers=self.METADATA_HEADERS,
                timeout=1,
            )
            return response.status_code == 200
        except Exception:
            return False

    def register_callback(self, callback: Callable[[PreemptionContext], None]) -> None:
        """Register callback for preemption notifications."""
        self._callbacks.append(callback)

    async def start(self) -> None:
        """Start preemption monitoring."""
        if self._running:
            return

        self._running = True

        # Setup SIGTERM handler
        self._setup_signal_handler()

        # Start monitoring task
        self._monitor_task = asyncio.create_task(self._monitor_loop())

        _get_logger().info(
            f"üõ°Ô∏è PreemptionDetector started "
            f"(GCP: {self._is_on_gcp}, interval: {self.poll_interval}s)"
        )

    async def stop(self) -> None:
        """Stop preemption monitoring."""
        self._running = False

        if self._monitor_task:
            self._monitor_task.cancel()
            try:
                await self._monitor_task
            except asyncio.CancelledError:
                pass
            self._monitor_task = None

        # Restore original signal handler
        if self._original_sigterm_handler is not None:
            signal.signal(signal.SIGTERM, self._original_sigterm_handler)

        _get_logger().info("PreemptionDetector stopped")

    def _setup_signal_handler(self) -> None:
        """Setup SIGTERM handler for graceful shutdown."""
        def sigterm_handler(signum, frame):
            _get_logger().warning("üö® SIGTERM received!")

            # Create preemption context
            context = PreemptionContext(
                signal_type=PreemptionSignal.SIGTERM,
                detected_at=time.time(),
                time_remaining=10.0,  # Assume ~10s for SIGTERM
            )

            # Store context
            self._preemption_context = context

            # Notify callbacks synchronously
            for callback in self._callbacks:
                try:
                    callback(context)
                except Exception as e:
                    _get_logger().error(f"Callback error: {e}")

        self._original_sigterm_handler = signal.signal(signal.SIGTERM, sigterm_handler)

    async def _monitor_loop(self) -> None:
        """Main monitoring loop."""
        while self._running:
            try:
                self._total_checks += 1

                # Check for preemption signals
                context = await self._check_all_signals()

                if context:
                    async with self._lock:
                        if self._preemption_context is None:
                            self._preemption_context = context
                            await self._notify_callbacks(context)

                await asyncio.sleep(self.poll_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                _get_logger().debug(f"Monitor loop error: {e}")
                await asyncio.sleep(self.poll_interval)

    async def _check_all_signals(self) -> Optional[PreemptionContext]:
        """Check all preemption signals."""
        if not self._is_on_gcp:
            return None

        try:
            import aiohttp

            async with aiohttp.ClientSession() as session:
                # Check maintenance events first (longest warning)
                context = await self._check_maintenance_event(session)
                if context:
                    self._maintenance_events += 1
                    return context

                # Check spot termination time
                context = await self._check_spot_termination(session)
                if context:
                    return context

                # Check preempted endpoint
                context = await self._check_preempted(session)
                if context:
                    self._preemption_notices += 1
                    return context

        except ImportError:
            # Fallback to requests
            context = await self._check_signals_sync()
            return context
        except Exception as e:
            _get_logger().debug(f"Signal check error: {e}")

        return None

    async def _check_maintenance_event(
        self, session
    ) -> Optional[PreemptionContext]:
        """Check GCP maintenance events endpoint."""
        try:
            async with session.get(
                self.MAINTENANCE_ENDPOINT,
                headers=self.METADATA_HEADERS,
                timeout=aiohttp.ClientTimeout(total=1),
            ) as response:
                if response.status == 200:
                    event = await response.text()
                    if event and event != "NONE":
                        return PreemptionContext(
                            signal_type=PreemptionSignal.MAINTENANCE_EVENT,
                            detected_at=time.time(),
                            time_remaining=self.maintenance_warning_seconds,
                        )
        except Exception:
            pass
        return None

    async def _check_spot_termination(
        self, session
    ) -> Optional[PreemptionContext]:
        """Check spot VM termination time endpoint."""
        try:
            async with session.get(
                self.SPOT_TERMINATION_ENDPOINT,
                headers=self.METADATA_HEADERS,
                timeout=aiohttp.ClientTimeout(total=1),
            ) as response:
                if response.status == 200:
                    termination_time = await response.text()
                    if termination_time:
                        # Parse ISO timestamp and calculate remaining time
                        try:
                            term_dt = datetime.fromisoformat(termination_time.replace("Z", "+00:00"))
                            remaining = (term_dt - datetime.now(term_dt.tzinfo)).total_seconds()
                            if remaining > 0:
                                return PreemptionContext(
                                    signal_type=PreemptionSignal.SPOT_TERMINATION,
                                    detected_at=time.time(),
                                    time_remaining=remaining,
                                )
                        except Exception:
                            pass
        except Exception:
            pass
        return None

    async def _check_preempted(
        self, session
    ) -> Optional[PreemptionContext]:
        """Check preempted endpoint."""
        try:
            async with session.get(
                self.PREEMPTED_ENDPOINT,
                headers=self.METADATA_HEADERS,
                timeout=aiohttp.ClientTimeout(total=1),
            ) as response:
                if response.status == 200:
                    text = await response.text()
                    if text.strip().upper() == "TRUE":
                        return PreemptionContext(
                            signal_type=PreemptionSignal.PREEMPTION_NOTICE,
                            detected_at=time.time(),
                            time_remaining=self.preemption_warning_seconds,
                        )
        except Exception:
            pass
        return None

    async def _check_signals_sync(self) -> Optional[PreemptionContext]:
        """Synchronous fallback using requests."""
        try:
            import requests

            # Check preempted endpoint
            response = requests.get(
                self.PREEMPTED_ENDPOINT,
                headers=self.METADATA_HEADERS,
                timeout=1,
            )
            if response.status_code == 200 and response.text.strip().upper() == "TRUE":
                return PreemptionContext(
                    signal_type=PreemptionSignal.PREEMPTION_NOTICE,
                    detected_at=time.time(),
                    time_remaining=self.preemption_warning_seconds,
                )
        except Exception:
            pass
        return None

    async def _notify_callbacks(self, context: PreemptionContext) -> None:
        """Notify all registered callbacks."""
        _get_logger().warning(
            f"üö® PREEMPTION DETECTED: {context.signal_type.value} "
            f"(~{context.time_remaining:.1f}s remaining)"
        )

        for callback in self._callbacks:
            try:
                if asyncio.iscoroutinefunction(callback):
                    await callback(context)
                else:
                    callback(context)
            except Exception as e:
                _get_logger().error(f"Callback error: {e}")

    def is_preemption_pending(self) -> bool:
        """Check if preemption is pending."""
        return self._preemption_context is not None

    def get_preemption_context(self) -> Optional[PreemptionContext]:
        """Get current preemption context."""
        return self._preemption_context

    def trigger_manual_preemption(self) -> None:
        """Manually trigger preemption (for testing)."""
        context = PreemptionContext(
            signal_type=PreemptionSignal.MANUAL,
            detected_at=time.time(),
            time_remaining=60.0,
        )
        self._preemption_context = context

        # Notify callbacks in background
        asyncio.create_task(self._notify_callbacks(context))

    def get_statistics(self) -> Dict[str, Any]:
        """Get monitoring statistics."""
        return {
            "is_on_gcp": self._is_on_gcp,
            "total_checks": self._total_checks,
            "maintenance_events": self._maintenance_events,
            "preemption_notices": self._preemption_notices,
            "is_running": self._running,
            "preemption_pending": self.is_preemption_pending(),
        }


# =============================================================================
# CHECKPOINT MANAGER
# =============================================================================


class CheckpointManager:
    """
    Advanced checkpoint manager with async operations and verification.
    """

    def __init__(
        self,
        checkpoint_dir: str,
        checkpoint_interval: int = 500,
        max_checkpoints: int = 3,
        gcs_bucket: Optional[str] = None,
        verify_checkpoints: bool = True,
        async_gcs_upload: bool = True,
    ):
        """
        Initialize checkpoint manager.

        Args:
            checkpoint_dir: Local directory for checkpoints
            checkpoint_interval: Save checkpoint every N steps
            max_checkpoints: Maximum number of checkpoints to keep
            gcs_bucket: Optional GCS bucket (e.g., gs://my-bucket/checkpoints)
            verify_checkpoints: Verify checkpoint integrity after save
            async_gcs_upload: Upload to GCS asynchronously
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_interval = checkpoint_interval
        self.max_checkpoints = max_checkpoints
        self.gcs_bucket = gcs_bucket
        self.verify_checkpoints = verify_checkpoints
        self.async_gcs_upload = async_gcs_upload

        # Create checkpoint directory
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)

        # GCS client (lazy init)
        self._gcs_client = None

        # Pending uploads
        self._pending_uploads: Deque[asyncio.Task] = deque()
        self._upload_lock = asyncio.Lock()

        # Metrics
        self._metrics = CheckpointMetrics()

        # Best checkpoint tracking
        self._best_loss = float("inf")
        self._best_checkpoint: Optional[Path] = None

        _get_logger().info(
            f"CheckpointManager initialized: {self.checkpoint_dir} "
            f"(interval: {checkpoint_interval}, GCS: {gcs_bucket is not None})"
        )

    def should_checkpoint(self, step: int) -> bool:
        """Check if we should save a checkpoint at this step."""
        return step > 0 and step % self.checkpoint_interval == 0

    async def save_checkpoint_async(
        self,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[Any] = None,
        global_step: int = 0,
        epoch: int = 0,
        best_loss: float = float("inf"),
        metadata: Optional[Dict[str, Any]] = None,
        checkpoint_type: CheckpointType = CheckpointType.REGULAR,
        training_config: Optional[Dict[str, Any]] = None,
        dataloader_state: Optional[Dict[str, Any]] = None,
        preemption_context: Optional[PreemptionContext] = None,
    ) -> str:
        """
        Save training checkpoint asynchronously.

        Args:
            model: Model to save
            optimizer: Optimizer to save
            scheduler: Optional scheduler to save
            global_step: Current training step
            epoch: Current epoch
            best_loss: Best validation loss so far
            metadata: Optional additional metadata
            checkpoint_type: Type of checkpoint
            training_config: Training configuration to save
            dataloader_state: DataLoader state for resumability
            preemption_context: Preemption context if emergency save

        Returns:
            Path to checkpoint directory
        """
        start_time = time.time()

        # Create checkpoint directory
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        checkpoint_name = f"checkpoint-step-{global_step}-{timestamp}"
        if checkpoint_type == CheckpointType.EMERGENCY:
            checkpoint_name = f"emergency-{checkpoint_name}"
        elif checkpoint_type == CheckpointType.BEST:
            checkpoint_name = f"best-{checkpoint_name}"

        checkpoint_path = self.checkpoint_dir / checkpoint_name
        checkpoint_path.mkdir(parents=True, exist_ok=True)

        _get_logger().info(
            f"üíæ Saving {checkpoint_type.value} checkpoint at step {global_step}..."
        )

        try:
            # Save model (with map_location for portability)
            model_path = checkpoint_path / "model.pt"
            await asyncio.to_thread(
                lambda: torch.save(
                    model.state_dict(),
                    model_path,
                    _use_new_zipfile_serialization=True,
                )
            )

            # Save optimizer
            optimizer_path = checkpoint_path / "optimizer.pt"
            await asyncio.to_thread(
                lambda: torch.save(optimizer.state_dict(), optimizer_path)
            )

            # Save scheduler if provided
            scheduler_path = None
            if scheduler is not None:
                scheduler_path = checkpoint_path / "scheduler.pt"
                await asyncio.to_thread(
                    lambda: torch.save(scheduler.state_dict(), scheduler_path)
                )

            # Save random states for full reproducibility
            random_states = self._capture_random_states()

            # Compute checksum
            checksum = CheckpointVerifier.compute_checkpoint_checksum(checkpoint_path)

            # Create checkpoint state
            state = CheckpointState(
                global_step=global_step,
                epoch=epoch,
                best_loss=best_loss,
                model_path=str(model_path),
                optimizer_state_path=str(optimizer_path),
                scheduler_state_path=str(scheduler_path) if scheduler_path else None,
                timestamp=datetime.now().isoformat(),
                metadata=metadata or {},
                checkpoint_type=checkpoint_type.value,
                resumability_status=ResumabilityStatus.COMPLETE.value,
                checksum=checksum,
                training_config=training_config or {},
                dataloader_state=dataloader_state,
                random_states=random_states,
                preemption_signal=(
                    preemption_context.signal_type.value if preemption_context else None
                ),
                time_to_preemption=(
                    preemption_context.time_remaining if preemption_context else None
                ),
            )

            # Save state JSON
            state_path = checkpoint_path / "checkpoint_state.json"
            await asyncio.to_thread(
                lambda: self._save_state_json(state_path, state)
            )

            # Verify checkpoint if enabled
            if self.verify_checkpoints:
                is_valid, error = CheckpointVerifier.verify_checkpoint(checkpoint_path)
                state.verification_passed = is_valid
                if not is_valid:
                    _get_logger().warning(f"Checkpoint verification failed: {error}")
                    self._metrics.verification_failures += 1
                else:
                    self._metrics.verification_passes += 1

                    # Update state with verification result
                    await asyncio.to_thread(
                        lambda: self._save_state_json(state_path, state)
                    )

            # Update metrics
            save_time = (time.time() - start_time) * 1000
            self._metrics.total_checkpoints += 1
            self._metrics.successful_saves += 1
            self._metrics.last_checkpoint_step = global_step
            self._metrics.last_checkpoint_time = time.time()
            self._metrics.avg_save_time_ms = (
                (self._metrics.avg_save_time_ms * (self._metrics.total_checkpoints - 1) + save_time)
                / self._metrics.total_checkpoints
            )

            if checkpoint_type == CheckpointType.EMERGENCY:
                self._metrics.emergency_saves += 1

            # Track best checkpoint
            if best_loss < self._best_loss:
                self._best_loss = best_loss
                self._best_checkpoint = checkpoint_path

            _get_logger().info(
                f"‚úÖ Checkpoint saved: {checkpoint_path.name} ({save_time:.1f}ms)"
            )

            # Upload to GCS
            if self.gcs_bucket:
                if self.async_gcs_upload:
                    task = asyncio.create_task(
                        self._upload_to_gcs_with_retry(checkpoint_path, state)
                    )
                    self._pending_uploads.append(task)
                    self._metrics.gcs_uploads_pending += 1
                else:
                    await self._upload_to_gcs_with_retry(checkpoint_path, state)

            # Cleanup old checkpoints
            await self._cleanup_old_checkpoints_async()

            return str(checkpoint_path)

        except Exception as e:
            self._metrics.failed_saves += 1
            _get_logger().error(f"‚ùå Checkpoint save failed: {e}")

            # Try to save minimal checkpoint in emergency
            if checkpoint_type == CheckpointType.EMERGENCY:
                try:
                    emergency_path = checkpoint_path / "emergency_model.pt"
                    torch.save(model.state_dict(), emergency_path)
                    _get_logger().info(f"Saved emergency model only: {emergency_path}")
                    return str(emergency_path)
                except Exception as e2:
                    _get_logger().error(f"Emergency save also failed: {e2}")

            raise

    def save_checkpoint(
        self,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[Any] = None,
        global_step: int = 0,
        epoch: int = 0,
        best_loss: float = float("inf"),
        metadata: Optional[Dict[str, Any]] = None,
    ) -> str:
        """Synchronous checkpoint save (legacy compatibility)."""
        # Run async version in new event loop if needed
        try:
            loop = asyncio.get_running_loop()
            future = asyncio.ensure_future(
                self.save_checkpoint_async(
                    model=model,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    global_step=global_step,
                    epoch=epoch,
                    best_loss=best_loss,
                    metadata=metadata,
                )
            )
            return loop.run_until_complete(future)
        except RuntimeError:
            # No running loop - create one
            return asyncio.run(
                self.save_checkpoint_async(
                    model=model,
                    optimizer=optimizer,
                    scheduler=scheduler,
                    global_step=global_step,
                    epoch=epoch,
                    best_loss=best_loss,
                    metadata=metadata,
                )
            )

    def _save_state_json(self, path: Path, state: CheckpointState) -> None:
        """Save state to JSON file."""
        state_dict = asdict(state)
        with open(path, "w") as f:
            json.dump(state_dict, f, indent=2, default=str)

    def _capture_random_states(self) -> Dict[str, Any]:
        """Capture all random states for reproducibility."""
        states = {}

        # Python random
        import random
        states["python_random"] = random.getstate()

        # NumPy random
        try:
            import numpy as np
            states["numpy_random"] = np.random.get_state()
        except ImportError:
            pass

        # PyTorch random
        states["torch_random"] = torch.get_rng_state().numpy().tolist()
        if torch.cuda.is_available():
            states["torch_cuda_random"] = [
                s.numpy().tolist() for s in torch.cuda.get_rng_state_all()
            ]

        return states

    def _restore_random_states(self, states: Dict[str, Any]) -> None:
        """Restore random states."""
        if not states:
            return

        import random
        import numpy as np

        if "python_random" in states:
            random.setstate(states["python_random"])

        if "numpy_random" in states:
            np.random.set_state(states["numpy_random"])

        if "torch_random" in states:
            torch.set_rng_state(torch.tensor(states["torch_random"]))

        if "torch_cuda_random" in states and torch.cuda.is_available():
            cuda_states = [torch.tensor(s) for s in states["torch_cuda_random"]]
            torch.cuda.set_rng_state_all(cuda_states)

    async def _upload_to_gcs_with_retry(
        self,
        checkpoint_path: Path,
        state: CheckpointState,
        max_retries: int = 3,
        base_delay: float = 1.0,
    ) -> bool:
        """Upload checkpoint to GCS with exponential backoff retry."""
        for attempt in range(max_retries):
            try:
                await self._upload_to_gcs(checkpoint_path)

                # Update state
                state.gcs_backup_status = "completed"
                state_path = checkpoint_path / "checkpoint_state.json"
                self._save_state_json(state_path, state)

                self._metrics.gcs_uploads_completed += 1
                self._metrics.gcs_uploads_pending -= 1

                _get_logger().info(
                    f"‚òÅÔ∏è Checkpoint uploaded to GCS: {checkpoint_path.name}"
                )
                return True

            except Exception as e:
                delay = base_delay * (2 ** attempt)
                _get_logger().warning(
                    f"GCS upload attempt {attempt + 1}/{max_retries} failed: {e}"
                    f" (retrying in {delay}s)"
                )

                if attempt < max_retries - 1:
                    await asyncio.sleep(delay)

        state.gcs_backup_status = "failed"
        state_path = checkpoint_path / "checkpoint_state.json"
        self._save_state_json(state_path, state)

        self._metrics.gcs_uploads_failed += 1
        self._metrics.gcs_uploads_pending -= 1

        _get_logger().error(f"GCS upload failed after {max_retries} attempts")
        return False

    async def _upload_to_gcs(self, checkpoint_path: Path) -> None:
        """Upload checkpoint to GCS."""
        try:
            from google.cloud import storage

            if self._gcs_client is None:
                self._gcs_client = storage.Client()

            # Parse bucket and path
            bucket_name = self.gcs_bucket.replace("gs://", "").split("/")[0]
            prefix = "/".join(self.gcs_bucket.replace("gs://", "").split("/")[1:])

            bucket = self._gcs_client.bucket(bucket_name)

            # Upload all files in checkpoint
            for file_path in checkpoint_path.rglob("*"):
                if file_path.is_file():
                    blob_name = f"{prefix}/{checkpoint_path.name}/{file_path.relative_to(checkpoint_path)}"
                    blob = bucket.blob(blob_name)

                    await asyncio.to_thread(
                        blob.upload_from_filename,
                        str(file_path),
                    )

        except ImportError:
            _get_logger().warning("google-cloud-storage not installed, skipping GCS upload")
            raise

    def load_latest_checkpoint(self) -> Optional[CheckpointState]:
        """Load the latest checkpoint."""
        # Find latest checkpoint
        checkpoints = list(self.checkpoint_dir.glob("checkpoint-*"))
        checkpoints.extend(self.checkpoint_dir.glob("emergency-*"))
        checkpoints.extend(self.checkpoint_dir.glob("best-*"))

        if not checkpoints:
            _get_logger().info("No checkpoints found")
            return None

        # Sort by modification time (most recent first)
        checkpoints.sort(key=lambda p: p.stat().st_mtime, reverse=True)

        # Try each checkpoint until we find a valid one
        for checkpoint in checkpoints:
            state_path = checkpoint / "checkpoint_state.json"

            if not state_path.exists():
                continue

            try:
                with open(state_path) as f:
                    state_dict = json.load(f)

                state = CheckpointState(**state_dict)

                # Verify if enabled
                if self.verify_checkpoints:
                    is_valid, error = CheckpointVerifier.verify_checkpoint(checkpoint)
                    if not is_valid:
                        _get_logger().warning(
                            f"Checkpoint {checkpoint.name} failed verification: {error}"
                        )
                        continue

                _get_logger().info(
                    f"üìÇ Loaded checkpoint from step {state.global_step}"
                )
                return state

            except Exception as e:
                _get_logger().warning(f"Failed to load checkpoint {checkpoint}: {e}")
                continue

        return None

    async def load_checkpoint_async(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        checkpoint_state: Optional[CheckpointState] = None,
        restore_random_states: bool = True,
    ) -> Optional[CheckpointState]:
        """
        Load checkpoint into model, optimizer, and scheduler asynchronously.
        """
        if checkpoint_state is None:
            checkpoint_state = self.load_latest_checkpoint()

        if checkpoint_state is None:
            return None

        # Load model
        _get_logger().info(f"Loading model from {checkpoint_state.model_path}")
        model_state = await asyncio.to_thread(
            torch.load,
            checkpoint_state.model_path,
            map_location="cpu",
        )
        model.load_state_dict(model_state)

        # Load optimizer
        if optimizer is not None and checkpoint_state.optimizer_state_path:
            _get_logger().info(
                f"Loading optimizer from {checkpoint_state.optimizer_state_path}"
            )
            optimizer_state = await asyncio.to_thread(
                torch.load,
                checkpoint_state.optimizer_state_path,
                map_location="cpu",
            )
            optimizer.load_state_dict(optimizer_state)

        # Load scheduler
        if scheduler is not None and checkpoint_state.scheduler_state_path:
            _get_logger().info(
                f"Loading scheduler from {checkpoint_state.scheduler_state_path}"
            )
            scheduler_state = await asyncio.to_thread(
                torch.load,
                checkpoint_state.scheduler_state_path,
                map_location="cpu",
            )
            scheduler.load_state_dict(scheduler_state)

        # Restore random states
        if restore_random_states and checkpoint_state.random_states:
            self._restore_random_states(checkpoint_state.random_states)

        _get_logger().info(
            f"‚úÖ Checkpoint loaded (step {checkpoint_state.global_step})"
        )

        return checkpoint_state

    def load_checkpoint(
        self,
        model: nn.Module,
        optimizer: Optional[torch.optim.Optimizer] = None,
        scheduler: Optional[Any] = None,
        checkpoint_state: Optional[CheckpointState] = None,
    ) -> Optional[CheckpointState]:
        """Synchronous checkpoint load (legacy compatibility)."""
        if checkpoint_state is None:
            checkpoint_state = self.load_latest_checkpoint()

        if checkpoint_state is None:
            return None

        # Load synchronously
        model.load_state_dict(torch.load(checkpoint_state.model_path))

        if optimizer is not None and checkpoint_state.optimizer_state_path:
            optimizer.load_state_dict(
                torch.load(checkpoint_state.optimizer_state_path)
            )

        if scheduler is not None and checkpoint_state.scheduler_state_path:
            scheduler.load_state_dict(
                torch.load(checkpoint_state.scheduler_state_path)
            )

        _get_logger().info(
            f"Checkpoint loaded successfully (step {checkpoint_state.global_step})"
        )
        return checkpoint_state

    async def _cleanup_old_checkpoints_async(self) -> None:
        """Remove old checkpoints, keeping only max_checkpoints most recent."""
        # Get regular checkpoints (not emergency or best)
        checkpoints = [
            p for p in self.checkpoint_dir.glob("checkpoint-step-*")
            if not p.name.startswith("emergency-") and not p.name.startswith("best-")
        ]

        if len(checkpoints) <= self.max_checkpoints:
            return

        # Sort by modification time (oldest first)
        checkpoints.sort(key=lambda p: p.stat().st_mtime)

        # Remove oldest checkpoints
        num_to_remove = len(checkpoints) - self.max_checkpoints
        for checkpoint in checkpoints[:num_to_remove]:
            _get_logger().info(f"üóëÔ∏è Removing old checkpoint: {checkpoint.name}")
            await asyncio.to_thread(shutil.rmtree, checkpoint)

    def _cleanup_old_checkpoints(self) -> None:
        """Synchronous cleanup (legacy)."""
        checkpoints = list(self.checkpoint_dir.glob("checkpoint-step-*"))

        if len(checkpoints) <= self.max_checkpoints:
            return

        checkpoints.sort(key=lambda p: int(p.name.split("-")[2].split("-")[0]))

        num_to_remove = len(checkpoints) - self.max_checkpoints
        for checkpoint in checkpoints[:num_to_remove]:
            _get_logger().info(f"Removing old checkpoint: {checkpoint}")
            shutil.rmtree(checkpoint)

    async def wait_for_pending_uploads(self, timeout: float = 60.0) -> int:
        """Wait for all pending GCS uploads to complete."""
        if not self._pending_uploads:
            return 0

        _get_logger().info(
            f"Waiting for {len(self._pending_uploads)} pending uploads..."
        )

        try:
            done, pending = await asyncio.wait(
                self._pending_uploads,
                timeout=timeout,
            )

            # Cancel any that didn't finish
            for task in pending:
                task.cancel()

            return len(done)

        except Exception as e:
            _get_logger().warning(f"Error waiting for uploads: {e}")
            return 0

    def get_metrics(self) -> Dict[str, Any]:
        """Get checkpoint manager metrics."""
        return asdict(self._metrics)


# =============================================================================
# SPOT VM CHECKPOINTER
# =============================================================================


class SpotVMCheckpointer(CheckpointManager):
    """
    Specialized checkpoint manager for GCP Spot VMs with predictive preemption.

    Features:
    - Multi-signal preemption detection
    - Automatic emergency checkpoint on preemption
    - Proactive checkpointing based on time remaining
    - GCS upload prioritization for emergency saves
    - Trinity integration for graceful shutdown coordination
    """

    def __init__(
        self,
        checkpoint_dir: str,
        checkpoint_interval: int = 500,
        gcs_bucket: Optional[str] = None,
        proactive_checkpoint_threshold: float = 45.0,  # Checkpoint if <45s remaining
        emergency_checkpoint_threshold: float = 15.0,  # Force checkpoint if <15s
    ):
        super().__init__(
            checkpoint_dir=checkpoint_dir,
            checkpoint_interval=checkpoint_interval,
            max_checkpoints=2,  # Keep only 2 for Spot VMs
            gcs_bucket=gcs_bucket,
            verify_checkpoints=True,
            async_gcs_upload=True,
        )

        self.proactive_checkpoint_threshold = proactive_checkpoint_threshold
        self.emergency_checkpoint_threshold = emergency_checkpoint_threshold

        # Preemption detection
        self._preemption_detector = PreemptionDetector()
        self._preemption_detected = False
        self._is_monitoring = False

        # Callbacks for preemption
        self._emergency_checkpoint_callback: Optional[Callable] = None

        # Track last checkpoint time for smart checkpointing
        self._last_checkpoint_time = time.time()
        self._min_checkpoint_interval = 30.0  # Don't checkpoint more than once per 30s

    async def start_monitoring(self) -> None:
        """Start monitoring for Spot VM preemption."""
        if self._is_monitoring:
            return

        self._is_monitoring = True

        # Register preemption callback
        self._preemption_detector.register_callback(self._handle_preemption)

        # Start detector
        await self._preemption_detector.start()

        _get_logger().info("üõ°Ô∏è SpotVMCheckpointer: Preemption monitoring started")

    async def stop_monitoring(self) -> None:
        """Stop monitoring for preemption."""
        self._is_monitoring = False
        await self._preemption_detector.stop()

        # Wait for pending uploads
        await self.wait_for_pending_uploads(timeout=30.0)

        _get_logger().info("SpotVMCheckpointer: Monitoring stopped")

    def start_monitoring_sync(self) -> None:
        """Synchronous version for legacy compatibility."""
        asyncio.create_task(self.start_monitoring())

    def _handle_preemption(self, context: PreemptionContext) -> None:
        """Handle preemption signal."""
        self._preemption_detected = True

        _get_logger().warning(
            f"üö® SPOT VM PREEMPTION: {context.signal_type.value} "
            f"({context.time_remaining:.1f}s remaining)"
        )

        # Trigger emergency checkpoint callback if registered
        if self._emergency_checkpoint_callback:
            try:
                self._emergency_checkpoint_callback(context)
            except Exception as e:
                _get_logger().error(f"Emergency checkpoint callback error: {e}")

    def register_emergency_checkpoint_callback(
        self,
        callback: Callable[[PreemptionContext], None],
    ) -> None:
        """Register callback for emergency checkpoint trigger."""
        self._emergency_checkpoint_callback = callback

    def should_checkpoint_proactive(self) -> bool:
        """
        Check if we should proactively checkpoint based on preemption signals.

        Returns True if:
        1. Preemption is pending with enough time remaining
        2. Haven't checkpointed recently
        """
        context = self._preemption_detector.get_preemption_context()

        if not context:
            return False

        # Check if we have time for proactive checkpoint
        if context.time_remaining < self.emergency_checkpoint_threshold:
            return False  # Too late for normal checkpoint

        if context.time_remaining < self.proactive_checkpoint_threshold:
            # Check minimum interval
            elapsed = time.time() - self._last_checkpoint_time
            if elapsed >= self._min_checkpoint_interval:
                return True

        return False

    def should_emergency_checkpoint(self) -> bool:
        """Check if we need emergency checkpoint."""
        context = self._preemption_detector.get_preemption_context()

        if not context:
            return False

        return context.time_remaining < self.emergency_checkpoint_threshold

    def is_preemption_detected(self) -> bool:
        """Check if preemption has been detected."""
        return self._preemption_detected or self._preemption_detector.is_preemption_pending()

    def get_time_remaining(self) -> Optional[float]:
        """Get estimated time remaining before preemption."""
        context = self._preemption_detector.get_preemption_context()

        if not context:
            return None

        elapsed = time.time() - context.detected_at
        return max(0, context.time_remaining - elapsed)

    async def save_emergency_checkpoint(
        self,
        model: nn.Module,
        optimizer: torch.optim.Optimizer,
        scheduler: Optional[Any] = None,
        global_step: int = 0,
        epoch: int = 0,
        best_loss: float = float("inf"),
        metadata: Optional[Dict[str, Any]] = None,
        training_config: Optional[Dict[str, Any]] = None,
    ) -> str:
        """
        Save emergency checkpoint with minimal overhead.

        Optimized for speed - skips some verification.
        """
        context = self._preemption_detector.get_preemption_context()

        _get_logger().warning("üö® Saving EMERGENCY checkpoint...")

        # Disable verification for speed
        old_verify = self.verify_checkpoints
        self.verify_checkpoints = False

        try:
            path = await self.save_checkpoint_async(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                global_step=global_step,
                epoch=epoch,
                best_loss=best_loss,
                metadata=metadata,
                checkpoint_type=CheckpointType.EMERGENCY,
                training_config=training_config,
                preemption_context=context,
            )

            self._last_checkpoint_time = time.time()

            # For emergency, do synchronous GCS upload if possible
            if self.gcs_bucket and context and context.time_remaining > 10:
                try:
                    await asyncio.wait_for(
                        self._upload_to_gcs(Path(path)),
                        timeout=min(context.time_remaining - 5, 30),
                    )
                except asyncio.TimeoutError:
                    _get_logger().warning("Emergency GCS upload timed out")

            return path

        finally:
            self.verify_checkpoints = old_verify

    def get_statistics(self) -> Dict[str, Any]:
        """Get comprehensive statistics."""
        stats = {
            "checkpoint_metrics": self.get_metrics(),
            "preemption_detector": self._preemption_detector.get_statistics(),
            "is_preemption_pending": self.is_preemption_detected(),
            "time_remaining": self.get_time_remaining(),
            "last_checkpoint_time": self._last_checkpoint_time,
        }
        return stats


# =============================================================================
# TRAINING CONTEXT MANAGER
# =============================================================================


@asynccontextmanager
async def spot_vm_training_context(
    checkpoint_dir: str,
    gcs_bucket: Optional[str] = None,
    checkpoint_interval: int = 500,
):
    """
    Context manager for Spot VM training with automatic preemption handling.

    Usage:
        async with spot_vm_training_context("./checkpoints") as checkpointer:
            for step in range(1000):
                train_step()

                if checkpointer.should_checkpoint(step):
                    await checkpointer.save_checkpoint_async(...)

                if checkpointer.is_preemption_detected():
                    await checkpointer.save_emergency_checkpoint(...)
                    break
    """
    checkpointer = SpotVMCheckpointer(
        checkpoint_dir=checkpoint_dir,
        gcs_bucket=gcs_bucket,
        checkpoint_interval=checkpoint_interval,
    )

    try:
        await checkpointer.start_monitoring()
        yield checkpointer

    finally:
        await checkpointer.stop_monitoring()


# =============================================================================
# MAIN
# =============================================================================


if __name__ == "__main__":
    import logging
    logging.basicConfig(level=logging.INFO)

    async def main():
        # Create checkpointer
        checkpointer = SpotVMCheckpointer(
            checkpoint_dir="./checkpoints",
            checkpoint_interval=100,
            gcs_bucket="gs://my-training-bucket/checkpoints",
        )

        print("‚úÖ SpotVMCheckpointer v91.0 initialized")
        print(f"Checkpoint directory: {checkpointer.checkpoint_dir}")
        print(f"Checkpoint interval: {checkpointer.checkpoint_interval}")
        print(f"GCS bucket: {checkpointer.gcs_bucket}")

        # Start monitoring
        await checkpointer.start_monitoring()

        print("\nüìä Statistics:")
        stats = checkpointer.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # Stop monitoring
        await checkpointer.stop_monitoring()

    asyncio.run(main())
